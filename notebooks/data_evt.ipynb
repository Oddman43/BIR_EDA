{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b07aa5af",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The data used for this Exploratory Data Analysis comes from two sources. The first is a database in SQLite3 called `flow.db` that was generated by a CLI script named `flowmodoro.py` I used to track the time spent focusing during my study sessions. An important caveat about this data is that the time is spent strictly focusing without pauses. The other source is a CSV file extracted from Anki using the AnkiConnect add-on and the Python script `anki_info.py` to extract the review instances of the cards in the decks dedicated to the exam.\n",
    "\n",
    "The data evaluated for this analysis corresponds to the logs between June 16, 2024, and January 25, 2025 (the day of the exam). Furthermore, starting September 9, 2024, all time logged in `flow.db` can be assumed to have been logged while studying in the public library, excluding local holidays like September 11, 2024, weekends and Christmas break ranging from December 24, 2024, and January 7, 2025, whereas prior to that, I was studying at home\n",
    "## `flow.db` Data Structure\n",
    "\n",
    "The `flow.db` data is structured as follows, with three tables: `daily_log`, `projects`, and `break_level`.\n",
    "\n",
    "-   `daily_log` contains the data referring to each focus cycle entry and has the following columns:\n",
    "\n",
    "| Column        | DataType   | Description                                                                                                       |\n",
    "|---------------|------------|-------------------------------------------------------------------------------------------------------------------|\n",
    "| id            | int        | Primary key, used to identify the log entry                                                                      |\n",
    "| project_id    | int        | Foreign key, used to relate to a project                                                                         |\n",
    "| started       | str        | Date in YYYY-MM-DD HH:MM:SS+ZZ:ZZ format corresponding to the start datetime. This column has an Index.          |\n",
    "| ended         | str        | Date in YYYY-MM-DD HH:MM:SS+ZZ:ZZ format corresponding to the end datetime                                         |\n",
    "| mins_w        | int        | Number of minutes worked in a focus cycle                                                                         |\n",
    "| accomp        | str        | Description of specific tasks worked on                                                                            |\n",
    "\n",
    "The data in the `accomplished` column follows a specific structure. It usually begins with the primary activity performed during that cycle. The content of the `accomplished` column, depending on its first word, can be interpreted as follows:\n",
    "\n",
    "As part of the Bear Hunter System, which I use for learning or reviewing information:\n",
    "\n",
    "-   \"AIM\", \"SHOOT\", or \"SKIN\".\n",
    "-   \"PRESTUDY\".\n",
    "\n",
    "As part of Active Recall:\n",
    "\n",
    "-   \"BD\" or \"BRAIN\" signifies Brain Dump, a component of active recall.\n",
    "-   \"SIR\" stands for Spacing, Interleaving, and Retrieval, another aspect of active recall.\n",
    "-   \"EXAM\", \"FIR\", \"GOBIR\", or \"SIMULACRO\" signifies time spent taking practice exams.\n",
    "\n",
    "As part of New Anki Cards or Anki Managment:\n",
    "\n",
    "-   \"ANKI\" indicates time spent creating new Anki cards.\n",
    "-   ANKI can be followed by \"ACM\", that denotes Active Card Management, the time dedicated to managing Anki cards.\n",
    "\n",
    "As part of watching lectures:\n",
    "\n",
    "-   \"VIDEO\" or \"CLASE\" corresponds to watching an academy lecture.\n",
    "\n",
    "As part of Anki Reviews of the deck `02-EXAM_Qs`:\n",
    "\n",
    "-   \"OFICIALES\" time spent doing Anki reviews of `02-EXAM_Qs` deck.\n",
    "\n",
    "Empty strings or initial words in the `accomplished` column that do not adhere to this structure are to be expected.\n",
    "\n",
    "Finally for this analysis, the data in `projects` and `break_level` is not relevant because projects while it houses the project name, in this instance I already know that the project `BIR` corresponding to the data for this analysis is project_id = 2, and break_level is used internaly by the `flowmodoro.py` script.\n",
    "\n",
    "For more info in `flowmodoro.py` script or `flow.db`, visit the [GitHub repository](https://github.com/Oddman43/Flowmodoro)\n",
    "\n",
    "## `anki_info.csv` Data Structure\n",
    "\n",
    "### Anki Information\n",
    "\n",
    "Anki is a popular open-source flashcard app and a great tool for learners. While studying for the exam, I primarily used two decks: the first, named `01-BIR`, housed the flashcards I created while studying, and the second, named `02-EXAM_Qs`, contained the actual questions asked in the last 20 years of exams, plus relevant questions from the Chemistry, Pharmacy, and Medicine exams.\n",
    "\n",
    "The cards in the `01-BIR` deck could will have the following tags associated\n",
    "\n",
    "-   `00Daily::` followed by `numMonth::` and `YYYYMMDD` used to suspend the cards the same day.\n",
    "-   A string of the overview topic of the tag, like `DNA` or `Nervioso`.\n",
    "\n",
    "Cards with no tags can be expected\n",
    "\n",
    "The cards in the `02-EXAM_Qs` deck are identified by a tag corresponding to the exam, using the three-letter abbreviation (`BIR`, `QIR`, `FIR`, `MIR`) followed by `::` and the year the question was asked. Finally, this deck also contains cards with the tag `GOBIR`, which are image-occlusion cards made from pictures of the practice questions provided by an academy.\n",
    "\n",
    "In Anki, a Note is the source of information, like a template containing fields such as a question and its answer, while a Card is a specific question-and-answer pair generated from a Note. A single Note can generate multiple Cards, each testing a different aspect of the information. For this Exploratory Data Analysis, I will use Cards instead of Notes because this approach provides more granularity for Notes that have multiple associated Cards, allowing for a more detailed analysis of review performance.\n",
    "\n",
    "### `anki_info.py`\n",
    "\n",
    "The `anki_info.py` script connects with Anki using the code in `anki_connect.py` and grabs all the data regarding the review history of the notes in the decks `01-BIR` and `02-EXAM_Qs` and outputs a DataFrame.\n",
    "\n",
    "The `anki_info.csv` data has the following columns:\n",
    "\n",
    "| Column    | DataType | Description                                                                                                                               |\n",
    "|-----------|----------|-------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| note_id   | int      | Unique id for each note, derived from the UNIX timestamp in milliseconds of the creation moment                                           |\n",
    "| card_id   | int      | Unique id for each card, derived from the UNIX timestamp in milliseconds of the creation moment                                           |\n",
    "| tags      | str      | String representation of the list of tags associated with each note, as described in the Anki Information section above                   |\n",
    "| reviews   | str      | Contains a history of each review event for the card, stored as a list of dictionaries (explained in the next table)                     |\n",
    "\n",
    "For each review the data has the following columns:\n",
    "\n",
    "| Column      | DataType | Description                                                 |\n",
    "|-------------|----------|-------------------------------------------------------------|\n",
    "| unix_time   | int      | UNIX timestamp in ms of the review instance                |\n",
    "| pressed     | int      | Integer corresponding to the button pressed; being 1 = Again, 2 = Hard, 3 = Good, 4 = Easy|\n",
    "| review_offset | int      | Time spent reviewing the card in ms                       |\n",
    "\n",
    "On pressed column it can also be \"0\", meaning a reschedule of the card, review instances with this pressed need to be filtered out.\n",
    "\n",
    "## Objectives\n",
    "\n",
    "The main questions to ask with this Exploratory Data Analysis are:\n",
    "\n",
    "-   Analyze whether the time invested in creating anki cards is recovered in the long run by doing reviews and if so at what point this happens\n",
    "-   Which anki cards have cost me the most, both in terms of time spent and success rate? Does the deck influence it?\n",
    "-   Is there significant difference in study time between studying at home vs. the library?\n",
    "-   Investigate potential causes for observed peaks in study time, such as proximity to exams, changes in study strategies, or external factors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef51055",
   "metadata": {},
   "source": [
    "# Data Loading and Cleaning\n",
    "\n",
    "## Used libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6605fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import urllib.request\n",
    "from dataclasses import dataclass\n",
    "import sqlite3\n",
    "import os\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "    level= logging.INFO,\n",
    "    format= \"%(asctime)s - %(levelname)s - %(message)s\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f0b7492",
   "metadata": {},
   "source": [
    "## Loading data from Anki\n",
    "\n",
    "### Anki Connect script\n",
    "\n",
    "Copied from [Anki Connect documentation](https://git.sr.ht/~foosoft/anki-connect/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322c2def",
   "metadata": {},
   "outputs": [],
   "source": [
    "def request(action: str, **params) -> dict:\n",
    "    return {\"action\": action, \"params\": params, \"version\": 6}\n",
    "\n",
    "\n",
    "def invoke(action: str, **params):\n",
    "    requestJson = json.dumps(request(action, **params)).encode(\"utf-8\")\n",
    "    response = json.load(\n",
    "        urllib.request.urlopen(\n",
    "            urllib.request.Request(\"http://127.0.0.1:8765\", requestJson)\n",
    "        )\n",
    "    )\n",
    "    if len(response) != 2:\n",
    "        raise Exception(\"response has an unexpected number of fields\")\n",
    "    if \"error\" not in response:\n",
    "        raise Exception(\"response is missing required error field\")\n",
    "    if \"result\" not in response:\n",
    "        raise Exception(\"response is missing required result field\")\n",
    "    if response[\"error\"] is not None:\n",
    "        raise Exception(response[\"error\"])\n",
    "    return response[\"result\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0b6198",
   "metadata": {},
   "source": [
    "### Script to get card info\n",
    "\n",
    "#### Anki Card Object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357c42a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Anki_Card:\n",
    "    \"\"\"\n",
    "    AnkiCard Object\n",
    "    \"\"\"\n",
    "    nid: int\n",
    "    cid: int\n",
    "    tags: list\n",
    "    reviews: list[dict]\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        \"\"\"Runs function post init\n",
    "        \"\"\"\n",
    "        self._clean_reviews()\n",
    "\n",
    "    def _clean_reviews(self):\n",
    "        \"\"\"\n",
    "        For each card takes the apropiate fields\n",
    "        and stores them in a list of dicts\n",
    "        \"\"\"\n",
    "        clean: list = []\n",
    "        for rev in self.reviews:\n",
    "            clean_dict = {\n",
    "                \"unix_time\": rev[\"id\"],\n",
    "                \"pressed\": rev[\"ease\"],\n",
    "                \"time_review_offset\": rev[\"time\"],\n",
    "            }\n",
    "            clean.append(clean_dict)\n",
    "        self.reviews = clean\n",
    "        return\n",
    "\n",
    "    def format_save(self):\n",
    "        \"\"\"Formats the object as dict\n",
    "\n",
    "        Returns:\n",
    "            dict: contains object info\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"note_id\": self.nid,\n",
    "            \"card_id\": self.cid,\n",
    "            \"tags_list\": self.tags,\n",
    "            \"reviews\": self.reviews,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9aed62",
   "metadata": {},
   "source": [
    "#### Extracting data from Anki\n",
    "\n",
    "The invoke function is used to acces the cards info in the apropiate decks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa0bfd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "decks: list[str] = [\"01-BIR\", \"02-EXAM_Qs\"]\n",
    "cards: list = []\n",
    "\n",
    "for deck in decks:\n",
    "    info_reviews: dict = {\n",
    "        k: v\n",
    "        for k, v in invoke(\n",
    "            \"getReviewsOfCards\", cards=(invoke(\"findCards\", query=f\"deck:{deck}\"))\n",
    "        ).items()\n",
    "        if len(v) >= 1\n",
    "    }\n",
    "    cards_info: list[dict] = invoke(\n",
    "        \"cardsInfo\", cards=[int(cid) for cid, _ in info_reviews.items()]\n",
    "    )\n",
    "    for card_dic in cards_info:\n",
    "        card = Anki_Card(\n",
    "            card_dic[\"note\"],\n",
    "            card_dic[\"cardId\"],\n",
    "            invoke(\"getNoteTags\", note=card_dic[\"note\"]),\n",
    "            info_reviews[str(card_dic[\"cardId\"])],\n",
    "        )\n",
    "        cards.append(card)\n",
    "    logging.info(f\"Extraceted infor from {deck}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a56b887",
   "metadata": {},
   "source": [
    "Load the card data into a DataFrame and save as parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0242d6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_parquet_path: str = \"../data/raw_anki.parquet\"\n",
    "raw_cards_df: pd.DataFrame = pd.DataFrame([card.format_save() for card in cards])\n",
    "raw_cards_df.to_parquet(raw_parquet_path, index=False)\n",
    "logging.info(f\"Saved data into {raw_parquet_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914e5584",
   "metadata": {},
   "source": [
    "Validate that the file exists and head of the DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f6c591f",
   "metadata": {},
   "source": [
    "### Data Verification from Anki\n",
    "#### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c41ef38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 12:59:38,486 - INFO - No null values in raw_cads\n"
     ]
    }
   ],
   "source": [
    "raw_cards_df = pd.read_parquet(\"../data/raw_anki.parquet\")\n",
    "if raw_cards_df.isnull().sum().sum() != 0:\n",
    "    logging.info(\"Data Frame raw_cards_df has null values\")\n",
    "logging.info(\"No null values in raw_cads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b27cfbf",
   "metadata": {},
   "source": [
    "Empty tags in `tags_list`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf4350c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:00:09,749 - INFO - Missing tags, will be assigned 'BIR' tag\n"
     ]
    }
   ],
   "source": [
    "raw_cards_df[\"not_empty_tags\"] = raw_cards_df[\"tags_list\"].apply(lambda x: len(x) > 0)\n",
    "if raw_cards_df[\"not_empty_tags\"].value_counts().get(False, 0) > 0:\n",
    "    logging.info(f\"Missing tags, will be assigned 'BIR' tag\")\n",
    "    missing_tags = raw_cards_df[\"not_empty_tags\"] == False\n",
    "    raw_cards_df.loc[missing_tags, \"tags_list\"] = raw_cards_df.loc[missing_tags, \"tags_list\"].apply(lambda _: [\"00Daily\"])\n",
    "else:\n",
    "    logging.info(\"No missing tags\")\n",
    "raw_cards_df = raw_cards_df.drop(columns= [\"not_empty_tags\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fca00a7",
   "metadata": {},
   "source": [
    "### Data Transformation\n",
    "#### Adding Deck column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e6bde5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:00:47,179 - INFO - Created deck column from tag info\n"
     ]
    }
   ],
   "source": [
    "deck_tags: set = {\"BIR\", \"MIR\", \"QIR\", \"FIR\", \"GOBIR\", \"Simulacro\"}\n",
    "raw_cards_df[\"deck\"] = raw_cards_df[\"tags_list\"].apply(\n",
    "    lambda tags_list: \n",
    "    \"02_exam\" if any(any(deck_tag in tag for deck_tag in deck_tags) for tag in tags_list)\n",
    "    else \"01_own\"\n",
    ")\n",
    "logging.info(\"Created deck column from tag info\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b97c31",
   "metadata": {},
   "source": [
    "#### Unnesting reviews column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d471daa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-18 13:01:42,601 - INFO - Unnested reviews column\n",
      "2025-11-18 13:01:42,941 - INFO - Json normalized the reviews column\n"
     ]
    }
   ],
   "source": [
    "raw_cards_df_2: pd.DataFrame = raw_cards_df.explode(\"reviews\")\n",
    "logging.info(\"Unnested reviews column\")\n",
    "raw_cards_df_2 = pd.concat(\n",
    "    [\n",
    "        raw_cards_df_2.drop(columns=[\"reviews\"]).reset_index(drop=True),\n",
    "        pd.json_normalize(raw_cards_df_2[\"reviews\"]).reset_index(drop=True) # type: ignore\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "logging.info(\"Json normalized the reviews column\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33c0f86",
   "metadata": {},
   "source": [
    "Changing the review time to seconds and unix timestamp to datetime object plus renaming the columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02681bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cards_df_2[\"time_review_offset\"] = raw_cards_df_2[\"time_review_offset\"] / 1000\n",
    "raw_cards_df_2[\"unix_time\"] = pd.to_datetime(raw_cards_df_2[\"unix_time\"], unit = \"ms\")\n",
    "raw_cards_df_2 = raw_cards_df_2.rename(columns= {\"time_review_offset\": \"review_time_seconds\", \"unix_time\": \"date_reviewed\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc8df04",
   "metadata": {},
   "source": [
    "Adding date_day column for grouping later, and filtering by date with the cut off 2025-01-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc48a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_cards_df_2[\"date_day\"] = raw_cards_df_2[\"date_reviewed\"].dt.date\n",
    "date_cut_off: pd.Timestamp = pd.to_datetime(\"2025-01-26\")\n",
    "raw_cards_df_2 = raw_cards_df_2[raw_cards_df_2[\"date_reviewed\"] <= date_cut_off]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f786e1",
   "metadata": {},
   "source": [
    "Saving the clean data into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b567bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_cards_df: pd.DataFrame = raw_cards_df_2\n",
    "logging.info(\"Renamed columns and changed time_reviews into seconds and unix_time to datateime\")\n",
    "clean_cards_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e285b9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_parquet_path_anki: str = \"../data/clean_anki.parquet\"\n",
    "clean_cards_df.to_parquet(clean_parquet_path_anki)\n",
    "if not os.path.exists(clean_parquet_path_anki):\n",
    "    logging.error(f\"Parquet file in {clean_parquet_path_anki} does not exist\")\n",
    "    raise FileExistsError(\"Parquet file was not created\")\n",
    "else:\n",
    "    logging.info(f\"Saved clean data in {clean_parquet_path_anki}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f908b18",
   "metadata": {},
   "source": [
    "## Loading data from Flowmodoro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4336040",
   "metadata": {},
   "source": [
    "### Extraction from SQLite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "665f3e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "db_path: str = \"../data/raw_flow.db\"\n",
    "with sqlite3.connect(db_path) as db:\n",
    "    cur: sqlite3.Cursor = db.cursor()\n",
    "    query: str = \"SELECT started, mins_worked, accomplished FROM daily_log WHERE project_id = 2;\"\n",
    "    result = cur.execute(query)\n",
    "    raw_flowmodoro_df: pd.DataFrame = pd.DataFrame(cur.fetchall(), columns= [\"started\", \"mins_worked\", \"accomplished\"]) \n",
    "\n",
    "raw_flowmodoro_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8c69a2",
   "metadata": {},
   "source": [
    "### Data verification for Flowmodoro data\n",
    "#### Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24378da",
   "metadata": {},
   "outputs": [],
   "source": [
    "if raw_flowmodoro_df.isnull().sum().sum() != 0:\n",
    "    logging.info(\"Data Frame raw_cards_df has null values\")\n",
    "logging.info(\"No null values in raw_cads\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7fc28ac",
   "metadata": {},
   "source": [
    "### Data Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d45d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_flowmodoro_df[\"started\"] = pd.to_datetime(raw_flowmodoro_df[\"started\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4857e63c",
   "metadata": {},
   "source": [
    "To represent the principal activity done in a cycle, the group describen in the Introduction with the following column names:\n",
    "\n",
    "- Bear Hunter System denoted as `bhs`\n",
    "- Time spent creating new Anki cards as `anki_craft`\n",
    "- Time spent doing active recall as `active_recall`\n",
    "- Time spent watching lectures as `lectures`\n",
    "- Time spent doing Anki reviews of the deck 02_EXAM_Qs as `anki_exam`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7313080c",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_flowmodoro_df[\"working_on\"] = raw_flowmodoro_df[\"accomplished\"].str.split().str[0]\n",
    "subs_dict: dict = {\n",
    "    \"AIM\": \"bhs\",\n",
    "    \"SHOOT\": \"bhs\",\n",
    "    \"SKIN\": \"bhs\",\n",
    "    \"PRESTUDY\": \"bhs\",\n",
    "    \"BD\": \"active_recall\",\n",
    "    \"SIR\": \"active_recall\",\n",
    "    \"EXAM\": \"active_recall\",\n",
    "    \"FIR\": \"active_recall\",\n",
    "    \"GOBIR\": \"active_recall\",\n",
    "    \"SIMULACRO\": \"active_recall\",\n",
    "    \"BRAIN\": \"active_recall\",\n",
    "    \"ANKI\": \"anki_craft\",\n",
    "    \"VIDEO\": \"lectures\",\n",
    "    \"CLASE\": \"lectures\",\n",
    "    \"OFICIALES\": \"anki_exam\"\n",
    "}\n",
    "raw_flowmodoro_df[\"working_on\"] = raw_flowmodoro_df[\"working_on\"].str.upper().map(subs_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37c0b8f",
   "metadata": {},
   "source": [
    "Now lets check for missing values in working_on and check for missed spellings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e53ea082",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    raw_flowmodoro_df.loc[\n",
    "        raw_flowmodoro_df[[\"accomplished\", \"working_on\"]].isnull().any(axis=1),\n",
    "        [\"accomplished\", \"working_on\"]\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc90a94f",
   "metadata": {},
   "source": [
    "The following ids correspond to the working_on:\n",
    "\n",
    "- bhs: 0, 1, 2, 3, 4, 6, 7, 12, 113, 224, 273, 385, 594, 961\n",
    "- active_recall: 413, 758, 977\n",
    "- anki_exam: 412, 754\n",
    "- admin: 93, 103, 142, 219, 401, 407"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4835dfc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_flowmodoro_df.loc[\n",
    "    [0, 1, 2, 3, 4, 6, 7, 12, 113, 224, 273, 385, 594, 961],\n",
    "    \"working_on\"\n",
    "] = \"bhs\"\n",
    "raw_flowmodoro_df.loc[\n",
    "    [413, 758, 977],\n",
    "    \"working_on\"\n",
    "] = \"active_recall\"\n",
    "raw_flowmodoro_df.loc[\n",
    "    [412, 754],\n",
    "    \"working_on\"\n",
    "] = \"anki_exam\"\n",
    "raw_flowmodoro_df.loc[\n",
    "    [93, 103, 142, 219, 401, 407],\n",
    "    \"working_on\"\n",
    "] = \"admin\"\n",
    "\n",
    "clean_flowmodoro_df: pd.DataFrame = raw_flowmodoro_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b651d9",
   "metadata": {},
   "source": [
    "Saving the clean data into a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8162d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_parquet_path_flowmodoro: str = \"../data/clean_flowmodoro.parquet\"\n",
    "clean_flowmodoro_df.to_parquet(clean_parquet_path_flowmodoro)\n",
    "if not os.path.exists(clean_parquet_path_flowmodoro):\n",
    "    logging.error(f\"Parquet file in {clean_parquet_path_flowmodoro} does not exist\")\n",
    "    raise FileExistsError(\"Parquet file was not created\")\n",
    "else:\n",
    "    logging.info(f\"Saved clean data in {clean_parquet_path_flowmodoro}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a594d72",
   "metadata": {},
   "source": [
    "## Creating a merged DataFrame\n",
    "Reading from clean data parquet files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a90f317",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowmodoro: pd.DataFrame = pd.read_parquet(\"../data/clean_flowmodoro.parquet\")\n",
    "anki: pd.DataFrame = pd.read_parquet(\"../data/clean_anki.parquet\")\n",
    "logging.info(\"Loaded clean data parquet files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4293aa2a",
   "metadata": {},
   "source": [
    "Grouping anki info by date and deck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200a414",
   "metadata": {},
   "outputs": [],
   "source": [
    "anki_grouped: pd.DataFrame = anki.groupby([\"date_day\", \"deck\"])[\"review_time_seconds\"].sum().reset_index()\n",
    "anki_grouped[\"review_time_mins\"] = anki_grouped[\"review_time_seconds\"] / 60\n",
    "anki_grouped = anki_grouped.drop(\"review_time_seconds\", axis= 1)\n",
    "logging.info(\"Created anki_gropued\")\n",
    "anki_grouped.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dc534d9",
   "metadata": {},
   "source": [
    "Grouping flowmodoro info by date and activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b151a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "flowmodoro[\"date_day\"] = flowmodoro[\"started\"].dt.date\n",
    "flowmodoro_grouped: pd.DataFrame = flowmodoro.groupby([\"date_day\", \"working_on\"])[\"mins_worked\"].sum().reset_index()\n",
    "logging.info(\"Created flowmodoro_grouped\")\n",
    "flowmodoro_grouped.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0c72cc",
   "metadata": {},
   "source": [
    "Renaming cols so they are the same in both df and concatenating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37c0924",
   "metadata": {},
   "outputs": [],
   "source": [
    "anki_grouped.columns = [\"date_day\", \"activity\", \"mins\"]\n",
    "flowmodoro_grouped.columns = [\"date_day\", \"activity\", \"mins\"]\n",
    "bir_time: pd.DataFrame = pd.concat([anki_grouped, flowmodoro_grouped], ignore_index= False)\n",
    "logging.info(\"Joined Anki and Flowmodoro data\")\n",
    "bir_time.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb41d69",
   "metadata": {},
   "source": [
    "Pivoting wider and filling NA with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf0e038",
   "metadata": {},
   "outputs": [],
   "source": [
    "bir_time_wide: pd.DataFrame = bir_time.pivot(index= \"date_day\", columns= \"activity\", values = \"mins\").fillna(0)\n",
    "logging.info(\"Pivoted wider bir time\")\n",
    "bir_time_wide.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c4f698",
   "metadata": {},
   "source": [
    "Since the time spent doing the 02_exam deck was double counted in anki_exam i will "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760e185d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bir_time_wide[\"exam_anki\"] = bir_time_wide[[\"02_exam\", \"anki_exam\"]].max(axis=1)\n",
    "bir_time_wide = bir_time_wide.drop(columns=[\"02_exam\", \"anki_exam\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "336ba494",
   "metadata": {},
   "source": [
    "Saving wide df into parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db49bb6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_bir_time_path: str = \"../data/clean_bir.parquet\"\n",
    "bir_time_wide.to_parquet(clean_bir_time_path)\n",
    "if not os.path.exists(clean_bir_time_path):\n",
    "    logging.error(f\"Parquet file in {clean_bir_time_path} does not exist\")\n",
    "    raise FileExistsError(\"Parquet file was not created\")\n",
    "else:\n",
    "    logging.info(f\"Saved clean data in {clean_bir_time_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bir",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
